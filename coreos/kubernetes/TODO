# Remove deprecations since v1.4.0

Oct 07 17:56:06 kubi-master0.novalocal kubelet-wrapper[16167]: Flag --config has been deprecated, Use --pod-manifest-path instead. Will be removed in a future version.
Oct 07 17:56:06 kubi-master0.novalocal kubelet-wrapper[16167]: Flag --api-servers has been deprecated, Use --kubeconfig instead. Will be removed in a future version.

# Use local dns in kube-proxy

Use the dns-mechanism from kube-apiserver in kube-proxy as well to get rid of this:

```
W1007 18:24:33.637123       1 server.go:417] Failed to retrieve node info: nodes "kubi-worker0.novalocal" not found
W1007 18:24:33.637337       1 proxier.go:226] invalid nodeIP, initialize kube-proxy with 127.0.0.1 as nodeIP
```

Therefor, `--hostname-override=$openstack servername$` must be set.

# Move etcd2 to fixed IPs (done)

The address-Space is large enough to use e.g. 10.1.0.1%index%

## Answer

Done

# Configure locksmith (partially done)

Currently there is no etcd cluster configured, but should be.
Maybe local etcd should be started in proxy-mode to real cluster?

## Answer

Done with auto updates enabled. This should be rethought on worker nodes, because they are not drained automatically.

# OpenStack Integration (done)

It is going to be harder than hoped… but as expected :(

For the integration to work, I need to hostname-override with e.g. kubi-master-kubernetes-master0.
But those ones are not available in the cluster. So kubectl logs breaks.

When trying to use cinder, controller-manager gave me: "wrong cloud type".
https://github.com/kubernetes/kubernetes/blob/master/pkg/volume/cinder/cinder.go#L187

But: openstack was used --cloud-provider=openstack and no earlier error occured. So I think the parameters are correct.
How to enable further debugging in kubelet?

## Answer

The option --cloud-provider has to go to manifests/kube-controller-manager.yaml as well

## pseudo DNS (done)
I need to rename the nodes from IP to openstacks NODENAME. The apiserver needs to be able to connect the nodes by this name.
A simple solution would be:
* expose all hosts with IP and NODENAME somewhere to etcd
* collect those etcd-keys on each node and construct a /etc/hosts
* on master, this /etc/hosts has to go into pod apiserver as well

More complex solution: real dns
But therefor I would need a search domain to be able to configure e.g. skydns with this search domain.

Done by real skydns on etcd nodes. Additional /etc/kubernetes/resolv.conf is shipped and hooked into kube-apiserver.


# on some etcd node, on a new setup: (done)
etcdctl set /coreos.com/network/config '{ "Network": "10.1.0.0/16" }'

Using vxlan does not include the flanneld to encapsulate the traffic, but the kernel will do. This may be a bit more performant.
etcdctl set /coreos.com/network/config '{ "Network": "10.1.0.0/16", "Backend": { "Type": "vxlan" } }'

host-gw is unfortunately not usable, because the underlying network in openstack, only routes traffic from known mac addresses.

## Answer

Done by cluster setup

# setting up kubectl(? or External IP on master)
https://coreos.com/kubernetes/docs/latest/configure-kubectl.html

## Answer

Exporting the api. So the setup can be on the users computer.
The cert only includes the DNS name "kubernetes-master". So you have to add this, with the associated floating ip, into your /etc/hosts

# cluster-ip... whats that? configured by me, is not valid (done)
core@kubernetes-master ~ $ kubectl get services
NAME         CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes   10.254.254.1   <none>        443/TCP   1h
             ^^^^^^^^^^^^(?)

## Answer

Exposed services are listening on this addresses.
http://kubernetes.io/docs/user-guide/services/

These are available from all nodes.

Changed to 10.3.0.0/24

# Create namespace "kube-system" (done)
ERROR: namespaces "kube-system" not found
http://kubernetes.io/docs/admin/namespaces/

## Answer

Nothing TODO on kubernetes 1.3+

Currently, this must be done by hand after the bootstrap.
kubectl create namespace kube-system

After a while, the kubernetes pods show up there.
kubectl get pods --namespace kube-system

# External IPs
To be able to schedule stuff to external reachable nodes.
Or to route stuff to external nodes to some pod.
http://kubernetes.io/docs/user-guide/services/

## Answer

So far, the Sys11Stack (OpenStack) does not support LBaaS. This means, it is not possible to use External Load Balancer. The only way, I got "public" services, is the usage of "NodePort". With the drawback, that a custom port is diced. But the service is available over each public ip on this port.

kubectl expose rc webserver-rc --port=30001 --target-port=80 --type=NodePort

# Multi Master Setup

service ip for internal communication

# Per worker tls certs

optional

# Networking

* split network to management network with etcd?
* Hard coded ips for etcd, may result in duplication of OS::Nova::Server, instead of using a count on a resource group

# missing
* calico (… do not need(?))
